#!/bin/bash
#BSUB -J feat_sel[1-32]                		# Job array name and job indexes, modify it according to the number of animals you want to analyze
#BSUB -n 10                            		# Number of tasks
#BSUB -R "span[hosts=1]"               		# Allocate all tasks in 1 host
#BSUB -q short                         		# Select queue
#BSUB -o output-%J-featSelection-.out          # Output file
#BSUB -e output-%J-featSelection-.err          # Error file

# Run here appropriate module load statements for your cluster environment
# module load ...

# Parameters
data_path=/path/to/experiment/folder
# Videos to process (all will run in parallel)
group1=WT
group2=KO
n_features=50
beam=beam1
shuffle=False
random_state=42
date=15042024
rfe_step=0.05
n_jobs=-1
indx=({1..100})

# Indicate which file to process
# Note: Manual now -> Switch to LSF array
idx=$LSB_JOBINDEX-1

# Set command-line arguments
df_data=$data_path/Results/$beam/feature_df.csv
folder_path=$data_path/Results/$beam
test_indx=${indx[$idx]}
echo "test_index: $test_indx"

# Call the Python script
python feature_selection.py \
        --df_data $df_data \
	--folder_path $folder_path \
	--test_indx $test_indx \
	--beam $beam\
	--group1 $group1\
	--group2 $group2\
	--n_features $n_features\
	--random_state $random_state\
	--shuffle $shuffle\
	--date $date\
	--n_jobs $n_jobs\
	--rfe_step $rfe_step
