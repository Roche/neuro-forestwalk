{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdca0f38-c8d4-41d9-8096-9e7cda6d3601",
   "metadata": {},
   "source": [
    "# Determining Vertical and Horizontal Parameters for Automated Foot Slip Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85252c",
   "metadata": {},
   "source": [
    "Copyright 2024 F. Hoffmann-La Roche AG\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62918387",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import auxiliaryFunctions as af\n",
    "from utils import beam_walk as bw\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ab781-a64b-4da6-9458-9269af09ad0b",
   "metadata": {},
   "source": [
    "## Retrieving DeepLabCut Tracking Data and Human-Annotated Foot Slip Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86896784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the folder path where the video files are located\n",
    "folder_path = 'path/to/threshold_dataset'  # Replace with your actual folder path\n",
    "string_to_exclude = 'labels'\n",
    "\n",
    "# Initialize Path object and file extensions\n",
    "p = Path(folder_path)\n",
    "\n",
    "# Collect all video file names without the extension\n",
    "files = [file.stem for file in p.glob('*.csv') if string_to_exclude not in file.stem]\n",
    "labeled_files = [file.stem for file in p.glob('*labels.csv')]\n",
    "\n",
    "# Initialize dataframes to store DeepLabCut predictions and target data\n",
    "total_dlc_df = pd.DataFrame()\n",
    "total_target_df = pd.DataFrame()\n",
    "\n",
    "# Initialize list and counters for foot slip analysis\n",
    "dist_list = []\n",
    "fs_n = 0  # Total number of foot slips\n",
    "f_videos = 0  # Number of videos with foot slips\n",
    "double_fs = 0  # Number of double foot slips\n",
    "\n",
    "# Process each video file\n",
    "for i, file in enumerate(files):\n",
    "    \n",
    "    if file.replace('_filtered', '_labels') in labeled_files:\n",
    "        print(f'Analyzing file: {file}')\n",
    "\n",
    "        # Extract DeepLabCut predictions and target data using custom functions\n",
    "        dlc_df, target_df = af.DLCandTarget(folder_path, file)\n",
    "\n",
    "        # Calculate the coordinates of the top edge of the beam\n",
    "        m, c = af.calculateBeamCoord(dlc_df)\n",
    "\n",
    "        # Normalize DeepLabCut dataframe by removing the beam y coordinates\n",
    "        norm_dlc_df = af.removeBeamCoord(dlc_df, m, c)\n",
    "\n",
    "        # Append target data and remove low likelihood points\n",
    "        norm_dlc_df['target'] = target_df\n",
    "        correct_norm_dlc_df = af.removeLowLikelihood(norm_dlc_df)\n",
    "\n",
    "        # Separate the target column into a different dataframe\n",
    "        correct_dlc_df = correct_norm_dlc_df.iloc[:, :-1]\n",
    "        correct_target_df = correct_norm_dlc_df.iloc[:, -1]\n",
    "\n",
    "        # Analyze foot slips\n",
    "        single_fs_n, single_fs_idx = af.analyzeFootSlips(correct_target_df)\n",
    "\n",
    "        # Update total number of foot slips\n",
    "        fs_n += single_fs_n\n",
    "\n",
    "        # Check for videos with more than one foot slip\n",
    "        if single_fs_n > 1:\n",
    "            f_videos = f_videos + 1\n",
    "            double_fs = single_fs_n\n",
    "            dist_fs = np.diff(single_fs_idx)\n",
    "\n",
    "            # Append distances between foot slips to the list\n",
    "            dist_list.extend(dist_fs)\n",
    "\n",
    "\n",
    "        # Concatenate dataframes from different videos\n",
    "        if i == 0:\n",
    "            total_dlc_df = correct_dlc_df\n",
    "            total_target_df = correct_target_df\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            total_dlc_df = pd.concat([total_dlc_df, correct_dlc_df], axis=0, ignore_index=True)\n",
    "            total_target_df = pd.concat([total_target_df, correct_target_df], axis=0, ignore_index=True)\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "# After processing all videos, you can now work with total_dlc_df and total_target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6a196-0132-425a-8a1a-7231d045b5f2",
   "metadata": {},
   "source": [
    "## Investigating the distribution of time intervals between distinct foot slips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame(dist_list, columns=['distance'])\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "sns.set(font_scale=10)\n",
    "sns.set(style='white')\n",
    "counts, bins = np.histogram(dist_list, bins = 100)\n",
    "\n",
    "b = sns.histplot(data=dist_list, bins=100, kde=True, element=\"step\", color='#A0B9C6', line_kws={'linewidth': 2})\n",
    "b.lines[0].set_color('#1E212B')\n",
    "\n",
    "sns.set(context='poster')\n",
    "b.set_xlabel(\"Distance in frames\",fontsize=20)\n",
    "b.set_ylabel(\"Counts\",fontsize=20)\n",
    "b.tick_params(labelsize=15)\n",
    "print('Selected horizontal threshold:', np.quantile(dist_list, 0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d7070-b088-4de9-b463-f20952330fb8",
   "metadata": {},
   "source": [
    "# Exploration for the Optimal Vertical Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e3f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the 'y' coordinate of the 'hindPaw' from the dataframe\n",
    "y_total_dlc_df = total_dlc_df[[('hindPaw', 'y')]]\n",
    "\n",
    "# Define a list of thresholds from 0 to 29\n",
    "thresholds = [x for x in range(30)]\n",
    "\n",
    "# Initialize lists to store TP, FN, and FP for each threshold\n",
    "TP_list = []\n",
    "FN_list = []\n",
    "FP_list = []\n",
    "\n",
    "# Define a threshold for the number of frames\n",
    "h_th = 32\n",
    "\n",
    "# Loop over each threshold\n",
    "for th in thresholds:\n",
    "    # Create a binary prediction where 1 indicates a foot slip (y-coordinate > threshold)\n",
    "    pred = y_total_dlc_df['hindPaw']>th\n",
    "    pred = pred*1\n",
    "\n",
    "    for i in range(pred.shape[0]-h_th):\n",
    "        if (pred['y'].iloc[i] == 1 and pred['y'].iloc[i+1] == 0):\n",
    "            if sum(pred['y'].iloc[i+1:i+10] > 0):\n",
    "                pred.loc[pred.index[i:i + h_th], 'y'] = 1\n",
    "\n",
    "    # Initialize counters for TP, FN, and FP\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    count = 0\n",
    "\n",
    "    # Loop over each frame, excluding the first frame\n",
    "    for i in range(1, pred.shape[0]):\n",
    "        # If a foot slip is predicted, increment the count by the ground truth value\n",
    "        if (pred['y'].iloc[i] == 1):\n",
    "            count = count + total_target_df[i]\n",
    "\n",
    "        # If a non-slip is predicted\n",
    "        if (pred['y'].iloc[i] == 0):\n",
    "            # If the previous frame was a foot slip\n",
    "            if pred['y'].iloc[i-1]:\n",
    "                # If the count is greater than 0, increment TP and reset count\n",
    "                if count > 0:\n",
    "                    TP = TP + 1\n",
    "                # If the count is 0, increment FP\n",
    "                else:\n",
    "                    FP = FP +1\n",
    "                count = 0\n",
    "\n",
    "    # Append TP and FP for the current threshold to their respective lists\n",
    "    TP_list.append(TP)\n",
    "    FP_list.append(FP)\n",
    "\n",
    "    # Initialize a counter for FN\n",
    "    count2 = 0\n",
    "\n",
    "    # Loop over each frame in the ground truth, excluding the first frame\n",
    "    for i in range(1, len(total_target_df)):\n",
    "        # If a foot slip is in the ground truth, increment the count by the prediction value\n",
    "        if total_target_df[i] == 1:\n",
    "            count2 = count2 + pred['y'].iloc[i]\n",
    "\n",
    "        # If a non-slip is in the ground truth\n",
    "        if total_target_df[i] == 0:\n",
    "            # If the previous frame was a foot slip\n",
    "            if total_target_df[i-1] == 1:\n",
    "                # If the count is 0, increment FN\n",
    "                if count2 == 0:\n",
    "                    FN = FN + 1\n",
    "            count2 = 0\n",
    "\n",
    "    # Append FN for the current threshold to its list\n",
    "    FN_list.append(FN)\n",
    "\n",
    "# Calculate recall, precision, and F1 score for each threshold\n",
    "Recall = np.array(TP_list)/(np.array(TP_list) + np.array(FN_list))\n",
    "Precision = np.array(TP_list)/(np.array(TP_list) + np.array(FP_list))\n",
    "f1_score = 2*(Precision*Recall)/(Precision+Recall)\n",
    "\n",
    "# Calculate the sum of recall and precision for each threshold\n",
    "R_plus_P = Recall + Precision\n",
    "# Find the maximum value of the sum\n",
    "max_value = max(R_plus_P)\n",
    "\n",
    "# Find the threshold(s) that give the maximum sum\n",
    "Best_threshold = [thresholds[index] for index, item in enumerate(R_plus_P) if item == max_value]\n",
    "\n",
    "# Print the best threshold and its recall and precision\n",
    "print('Selected vertical threshold:', Best_threshold)\n",
    "print('Recall:', Recall[Best_threshold])\n",
    "print('Precision:', Precision[Best_threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (9,9))\n",
    "sns.set(style='white')\n",
    "sns.despine()\n",
    "sns.scatterplot(x=Recall, y=Precision, palette='Blues', hue=thresholds, s=140)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.legend(title='Threshold (px)', loc='lower left', fontsize=20, title_fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f0ea0-d350-4d13-9277-ffb94a13a20d",
   "metadata": {},
   "source": [
    "# Validation of the selected parameters in new videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fc25b-de21-473c-b95e-b653eb7bf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected threshold\n",
    "h_th = 32 # Horizontal threshold, in frames\n",
    "y_th = 18 # Vertical threshold, in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26d234-ae73-4ae1-aa96-edb35db55878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = folder_path = 'path/to/validation_dataset'\n",
    "avi_files = [os.path.splitext(f)[0] for f in os.listdir(p) if f.endswith('.avi')]\n",
    "csv_files = [os.path.splitext(f)[0] for f in os.listdir(p) if f.endswith('.csv')]\n",
    "txt_files = [os.path.splitext(f)[0] for f in os.listdir(p) if f.endswith('.txt')]\n",
    "\n",
    "\n",
    "total_target = np.array([])\n",
    "total_dlc_df = pd.DataFrame(columns = ['hindPaw_y'])\n",
    "\n",
    "for i,fi in enumerate(avi_files):\n",
    "    dlc_df = pd.read_csv(os.path.join(p, fi + '.csv'), \n",
    "                         header=[1,2],\n",
    "                        dtype =np.float64, \n",
    "                        index_col=0)\n",
    "    m,c = af.calculateBeamCoord(dlc_df)\n",
    "    norm_dlc_df = af.removeBeamCoord(dlc_df, m, c)\n",
    "    \n",
    "    \n",
    "    if 'Result_' + fi in txt_files:\n",
    "        \n",
    "        \n",
    "        file_name = 'Result_' + fi\n",
    "        file_d = os.path.join(p, file_name + '.txt')\n",
    "        with open(file_d) as f:\n",
    "            r_lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        temp_scoring = np.zeros((norm_dlc_df.shape[0]))\n",
    "        # Interesting stuff is happening starting from line 2\n",
    "        for i in range(2,len(r_lines)):\n",
    "            r_content = r_lines[i].split()\n",
    "            temp_scoring[int(r_content[-3]):int(r_content[-2])] = 1\n",
    "\n",
    "    else:\n",
    "        \n",
    "        file_name = fi + '_labels.csv'\n",
    "        file_d = pd.read_csv(os.path.join(p, file_name), header=[0])\n",
    "        \n",
    "        if 'footslips' in file_d.columns:\n",
    "            temp_scoring = file_d['footslips']\n",
    "        else:\n",
    "            temp_scoring = file_d['footslip']\n",
    "        temp_scoring= np.array(temp_scoring)\n",
    "    \n",
    "    results_df = pd.DataFrame({'scoring':temp_scoring, 'hindPaw_y':norm_dlc_df[('hindPaw', 'y')], 'hindPaw_likelihood':norm_dlc_df[('hindPaw', 'likelihood')]})\n",
    "    results_df = results_df[results_df.hindPaw_likelihood > 0.95]\n",
    "    \n",
    "    total_target = np.concatenate((total_target, np.array(results_df.scoring)))\n",
    "    total_dlc_df = pd.concat([total_dlc_df, results_df[['hindPaw_y']]], axis=0, ignore_index=True)\n",
    "\n",
    "    \n",
    "TP_list = []\n",
    "FN_list = []\n",
    "FP_list = []\n",
    "\n",
    "\n",
    "pred = total_dlc_df> y_th\n",
    "pred = pred*1\n",
    "\n",
    "# (3) I need to incorporate the temporal threshold\n",
    "for i in range(pred.shape[0]-h_th):\n",
    "    if (pred['hindPaw_y'].iloc[i] == 1 and pred['hindPaw_y'].iloc[i+1] == 0):\n",
    "         if sum(pred['hindPaw_y'].iloc[i+1:i+h_th] > 0):\n",
    "                for j in range(h_th):\n",
    "                    pred['hindPaw_y'].iloc[i+j] = 1\n",
    "\n",
    "\n",
    "                    \n",
    "TP = 0\n",
    "FN = 0\n",
    "FP = 0\n",
    "count = 0\n",
    "\n",
    "# (4) I count the number of true positives\n",
    "\n",
    "for i in range(1, pred.shape[0]):\n",
    "    if (pred['hindPaw_y'].iloc[i] == 1):\n",
    "        count = count + total_target[i]\n",
    "\n",
    "    if (pred['hindPaw_y'].iloc[i] == 0):\n",
    "        if pred['hindPaw_y'].iloc[i-1]:\n",
    "            if count > 0:\n",
    "                TP = TP + 1\n",
    "            else:\n",
    "                FP = FP +1\n",
    "            count = 0\n",
    "            \n",
    "TP_list.append(TP)\n",
    "FP_list.append(FP)\n",
    "\n",
    "count2 = 0\n",
    "for i in range(1, len(total_target)):\n",
    "    if total_target[i] == 1:\n",
    "        count2 = count2 + pred['hindPaw_y'].iloc[i]\n",
    "\n",
    "    if total_target[i] == 0:\n",
    "        if total_target[i-1] == 1:\n",
    "            if count2 == 0:\n",
    "                FN = FN + 1\n",
    "        count2 = 0\n",
    "FN_list.append(FN)\n",
    "                    \n",
    "        \n",
    "Recall = np.array(TP_list)/(np.array(TP_list) + np.array(FN_list))\n",
    "Precision = np.array(TP_list)/(np.array(TP_list) + np.array(FP_list))\n",
    "f1_score = 2*(Precision*Recall)/(Precision+Recall)\n",
    "\n",
    "print('Recall:', Recall)\n",
    "print('Precision:', Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e17e35-9011-495e-925a-7d33ac6f0b34",
   "metadata": {},
   "source": [
    "# Agreement between scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ca720-5908-44d9-b9c4-8ebd802a956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "path = '/path/to/agreement_dataset'\n",
    "\n",
    "# Initialize lists to store the number of foot slips annotated by each scorer for each video\n",
    "bfs = [] # Scorer 4\n",
    "rfs = [] # Scorer 2\n",
    "mfs = [] # Scorer 3\n",
    "ffs = [] # Scorer 1\n",
    "\n",
    "# Get the names of all .mp4 files in the dataset directory\n",
    "p = Path(path)\n",
    "video_files = [i.stem for i in p.glob('**/*.mp4')]\n",
    "\n",
    "# Loop over each video\n",
    "for n, video in enumerate(video_files):\n",
    "    # Read the deeplabcut csv file for the current video\n",
    "    dlc_path = os.path.join(p, 'Result_' +video + '.csv')\n",
    "    dlc_df = pd.read_csv(dlc_path, header=[1,2], dtype =np.float64, index_col=0)\n",
    "\n",
    "    # Read the foot slip annotations for Scorer 1\n",
    "    f_scoring = pd.read_csv(os.path.join(p, 'Result_' + video + '_labels.csv'), header=[0], dtype =np.float64, index_col=0)['footslips']\n",
    "    f_scoring = np.array(f_scoring)\n",
    "\n",
    "    # Calculate the total number of foot slips annotated by Scorer 1\n",
    "    fFootslips = sum(f_scoring[i] == 1 and f_scoring[i-1] == 0 for i in range(1, len(f_scoring)))\n",
    "    ffs.append(fFootslips)\n",
    "\n",
    "    # Read the foot slip annotations for Scorer 2\n",
    "    with open(os.path.join(p, 'Result_' +  video + '_R.txt')) as f:\n",
    "        r_lines = f.readlines()\n",
    "    r_scoring = np.zeros((dlc_df.shape[0]))\n",
    "    for i in range(2,len(r_lines)):\n",
    "        r_content = r_lines[i].split()\n",
    "        r_scoring[int(r_content[-3]):int(r_content[-2])] = 1\n",
    "\n",
    "    # Calculate the total number of foot slips annotated by Scorer 2\n",
    "    rFootslips = sum(r_scoring[i] == 1 and r_scoring[i-1] == 0 for i in range(1, len(r_scoring)))\n",
    "    rfs.append(rFootslips)\n",
    "\n",
    "    # Read the foot slip annotations for Scorer 3\n",
    "    with open(os.path.join(p, 'Result_' + video + '_M.txt')) as f:\n",
    "        m_lines = f.readlines()\n",
    "    m_scoring = np.zeros((dlc_df.shape[0]))\n",
    "    for i in range(2,len(m_lines)):\n",
    "        m_content = m_lines[i].split()\n",
    "        m_scoring[int(m_content[-3]):int(m_content[-2])] = 1\n",
    "\n",
    "    # Calculate the total number of foot slips annotated by Scorer 3\n",
    "    mFootslips = sum(m_scoring[i] == 1 and m_scoring[i-1] == 0 for i in range(1, len(m_scoring)))\n",
    "    mfs.append(mFootslips)\n",
    "\n",
    "    # Read the foot slip annotations for Scorer 4\n",
    "    with open(os.path.join(p, 'Result_' + video + '_B.txt')) as f:\n",
    "        b_lines = f.readlines()\n",
    "    b_scoring = np.zeros((dlc_df.shape[0]))\n",
    "    for i in range(2,len(b_lines)):\n",
    "        b_content = b_lines[i].split()\n",
    "        b_scoring[int(b_content[-3]):int(b_content[-2])] = 1\n",
    "\n",
    "    # Calculate the total number of foot slips annotated by Scorer 4\n",
    "    bFootslips = sum(b_scoring[i] == 1 and b_scoring[i-1] == 0 for i in range(1, len(b_scoring)))\n",
    "    bfs.append(bFootslips)\n",
    "\n",
    "    # Concatenate the foot slip annotations from all scorers into a single dataframe\n",
    "    if n == 0:\n",
    "        scoring_df = pd.DataFrame({'S1':f_scoring, 'S2':r_scoring, 'S3':m_scoring, 'S4':b_scoring})\n",
    "    else:\n",
    "        prov_df = pd.DataFrame({'S1':f_scoring, 'S2':r_scoring, 'S3':m_scoring, 'S4':b_scoring})\n",
    "        scoring_df = pd.concat([scoring_df, prov_df], axis=0, ignore_index=True)\n",
    "\n",
    "# Create a dataframe with the total number of foot slips annotated by each scorer for each video\n",
    "foot_df = pd.DataFrame({'video':video_files, 'S1': ffs, 'S2':rfs, 'S3':mfs, 'S4':bfs})\n",
    "foot_df = foot_df.set_index('video', drop=True)\n",
    "\n",
    "# Transpose the dataframe\n",
    "new_df_t = foot_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2bd12-3696-4715-a8e2-feade195de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_pred = pd.DataFrame()\n",
    "tfs = []\n",
    "trial_list = []\n",
    "extension2 = '.csv'\n",
    "for video in video_files:\n",
    "    print(video)\n",
    "    video_path = os.path.join(p, video + '.mp4')\n",
    "    dlc_path = os.path.join(p, 'Result_' + video + extension2)\n",
    "    \n",
    "    dlc_df = pd.read_csv(\n",
    "        dlc_path, \n",
    "        header=[1,2], #setting the first 2 rows as column names\n",
    "        dtype =np.float64, #defining the type of the objects inside the dataframe\n",
    "        index_col=0\n",
    "    )\n",
    "    \n",
    "   \n",
    "    trial = af.Trial(video_path, dlc_df, 120)\n",
    "    \n",
    "    trial_list.append(trial)\n",
    "\n",
    "    tfs.append(trial.footslips)\n",
    "\n",
    "new_df_t.loc['Threshold'] = tfs\n",
    "\n",
    "transposed_df = new_df_t.T\n",
    "k = np.zeros((5,5))\n",
    "\n",
    "\n",
    "r=0\n",
    "for c in range(transposed_df.shape[1]):\n",
    "    j=0\n",
    "    for t in range(transposed_df.shape[1]):\n",
    "        kappa = cohen_kappa_score(transposed_df.iloc[:,c], transposed_df.iloc[:,t])\n",
    "        k[j][r] = kappa\n",
    "        j=j+1\n",
    "    r=r+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f7ddb-37ed-4a2a-a501-b2d5e4610880",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen = pd.DataFrame(\n",
    "    k,\n",
    "    columns = new_df_t.index,\n",
    "    index = new_df_t.index\n",
    ")\n",
    "\n",
    "cohen.to_csv('agreementCohenK.csv')\n",
    "sns.set(font_scale=2)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cohen,\n",
    "            square=True, linewidths=.5, cmap='crest', annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_environment",
   "language": "python",
   "name": "test_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
