#!/bin/bash
#BSUB -J classification[1-3]          		# Modify it according to the number of beams you want to analyze in parallel
#BSUB -n 3                          		# Number of tasks
#BSUB -R "span[hosts=1]"            		# Allocate all tasks in 1 host
#BSUB -q short                      		# Select queue
#BSUB -o output-%J-classification.out           # Output file
#BSUB -e output-%J-classification.err           # Error file

# Run here appropriate module load statements for your cluster environment
# module load ...

# Parameters
project_path=/path/to/experiment/folder
# Videos to process (all will run in parallel)
beams=(beam1 beam2 beam3)
group1=WT
group2=KO
shuffle=False
date=15042024_5
n_features=50
data=feature_df.csv
n_jobs=-1

# Indicate which file to process
# Note: Manual now -> Switch to LSF array
idx=$LSB_JOBINDEX-1

# Set command-line arguments
df_data=$project_path/Results/${beams[$idx]}/$data
results_path=$project_path/Results/${beams[$idx]}
folder_path=$project_path/Results/${beams[$idx]}
beam=${beams[$idx]}

echo "test_index: $test_indx"

# Call the Python script
python classification.py \
        --df_data $df_data \
	--folder_path $folder_path \
        --results_path $results_path\
	--group1 $group1 \
	--group2 $group2 \
	--beam $beam \
	--shuffle $shuffle\
	--date $date\
	--n_features $n_features\
	--data $data\
	--n_jobs $n_jobs
	
